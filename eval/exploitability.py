"""Exploitability computation for imperfect-information games.

Provides:
- Exact best-response via full tree traversal (Kuhn, Leduc)
- Local Best Response (LBR) sampling for large games (HUNL)
- Exploitability = 0.5 * (BR_value_p0 + BR_value_p1)

A strategy with exploitability 0 is a Nash equilibrium.
"""

from __future__ import annotations

import random
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple

from game_interface import Game

StrategyProfile = Dict[str, Dict[str, float]]


def exploitability(game: Game, profile: StrategyProfile) -> float:
    """Compute exploitability of a strategy profile.

    exploitability = 0.5 * (BR_value(p0) + BR_value(p1))

    where BR_value(p) is the expected value that a best-responding player p
    can achieve against the fixed opponent strategy.

    Returns 0 for a Nash equilibrium; positive otherwise.
    """
    br0 = best_response_value(game, profile, br_player=0)
    br1 = best_response_value(game, profile, br_player=1)
    return 0.5 * (br0 + br1)


def best_response_value(
    game: Game,
    profile: StrategyProfile,
    br_player: int,
) -> float:
    """Value of the best response for br_player against the fixed profile.

    Uses full tree traversal — exact for small games (Kuhn, Leduc).
    For large games, use lbr_exploitability instead.
    """
    state_reach: Dict = {}
    infoset_states: Dict[str, List] = {}
    infoset_actions: Dict[str, List] = {}

    def collect(state, reach_opp: float) -> None:
        state_reach[state] = reach_opp
        if game.is_terminal(state):
            return
        if game.current_player(state) == -1:
            for outcome, prob in game.chance_outcomes(state):
                collect(game.next_state(state, outcome), reach_opp * prob)
            return
        player = game.current_player(state)
        actions = game.legal_actions(state)
        if player == br_player:
            key = game.infoset_key(state, player)
            infoset_states.setdefault(key, []).append(state)
            infoset_actions.setdefault(key, actions)
            for a in actions:
                collect(game.next_state(state, a), reach_opp)
        else:
            key = game.infoset_key(state, player)
            action_probs = profile.get(key, {})
            for a in actions:
                p = action_probs.get(a, 1.0 / len(actions))
                collect(game.next_state(state, a), reach_opp * p)

    collect(game.initial_state(), 1.0)

    value_cache: Dict = {}
    action_cache: Dict[str, str] = {}

    def best_action(key: str) -> str:
        if key in action_cache:
            return action_cache[key]
        actions = infoset_actions[key]
        totals = [0.0] * len(actions)
        for state in infoset_states[key]:
            for idx, a in enumerate(actions):
                totals[idx] += state_reach[state] * state_value(game.next_state(state, a))
        best_idx = max(range(len(actions)), key=lambda i: totals[i])
        action_cache[key] = actions[best_idx]
        return actions[best_idx]

    def state_value(state) -> float:
        if state in value_cache:
            return value_cache[state]
        if game.is_terminal(state):
            v = game.terminal_utility(state, br_player)
        elif game.current_player(state) == -1:
            v = 0.0
            for outcome, prob in game.chance_outcomes(state):
                v += prob * state_value(game.next_state(state, outcome))
        else:
            player = game.current_player(state)
            actions = game.legal_actions(state)
            if player == br_player:
                key = game.infoset_key(state, player)
                a = best_action(key)
                v = state_value(game.next_state(state, a))
            else:
                key = game.infoset_key(state, player)
                action_probs = profile.get(key, {})
                v = 0.0
                for a in actions:
                    p = action_probs.get(a, 1.0 / len(actions))
                    v += p * state_value(game.next_state(state, a))
        value_cache[state] = v
        return v

    return state_value(game.initial_state())


def best_response_strategy(
    game: Game,
    profile: StrategyProfile,
    br_player: int,
) -> StrategyProfile:
    """Compute the full best-response strategy for br_player.

    Returns a StrategyProfile where br_player plays the maximally
    exploiting strategy and the opponent plays according to profile.
    """
    state_reach: Dict = {}
    infoset_states: Dict[str, List] = {}
    infoset_actions: Dict[str, List] = {}

    def collect(state, reach_opp: float) -> None:
        state_reach[state] = reach_opp
        if game.is_terminal(state):
            return
        if game.current_player(state) == -1:
            for outcome, prob in game.chance_outcomes(state):
                collect(game.next_state(state, outcome), reach_opp * prob)
            return
        player = game.current_player(state)
        actions = game.legal_actions(state)
        if player == br_player:
            key = game.infoset_key(state, player)
            infoset_states.setdefault(key, []).append(state)
            infoset_actions.setdefault(key, actions)
            for a in actions:
                collect(game.next_state(state, a), reach_opp)
        else:
            key = game.infoset_key(state, player)
            action_probs = profile.get(key, {})
            for a in actions:
                p = action_probs.get(a, 1.0 / len(actions))
                collect(game.next_state(state, a), reach_opp * p)

    collect(game.initial_state(), 1.0)

    value_cache: Dict = {}

    def state_value(state) -> float:
        if state in value_cache:
            return value_cache[state]
        if game.is_terminal(state):
            v = game.terminal_utility(state, br_player)
        elif game.current_player(state) == -1:
            v = 0.0
            for outcome, prob in game.chance_outcomes(state):
                v += prob * state_value(game.next_state(state, outcome))
        else:
            player = game.current_player(state)
            actions = game.legal_actions(state)
            if player == br_player:
                key = game.infoset_key(state, player)
                totals = [0.0] * len(actions)
                for s in infoset_states[key]:
                    for idx, a in enumerate(actions):
                        totals[idx] += state_reach[s] * state_value(game.next_state(s, a))
                best_idx = max(range(len(actions)), key=lambda i: totals[i])
                v = state_value(game.next_state(state, actions[best_idx]))
            else:
                key = game.infoset_key(state, player)
                action_probs = profile.get(key, {})
                v = 0.0
                for a in actions:
                    p = action_probs.get(a, 1.0 / len(actions))
                    v += p * state_value(game.next_state(state, a))
        value_cache[state] = v
        return v

    # Build the BR strategy profile
    br_profile: StrategyProfile = {}
    for key, actions in infoset_actions.items():
        totals = [0.0] * len(actions)
        for state in infoset_states[key]:
            for idx, a in enumerate(actions):
                totals[idx] += state_reach[state] * state_value(game.next_state(state, a))
        best_idx = max(range(len(actions)), key=lambda i: totals[i])
        br_profile[key] = {a: (1.0 if i == best_idx else 0.0) for i, a in enumerate(actions)}

    return br_profile


@dataclass
class LBRConfig:
    """Configuration for Local Best Response (LBR) sampling.

    LBR approximates exploitability by sampling game trajectories and
    computing best responses locally at each information set encountered.
    Suitable for large games where full tree traversal is infeasible.

    Reference: Lisy et al., "Online Monte Carlo Counterfactual Regret
    Minimization for Search in Imperfect Information Games" (2015)
    """
    num_samples: int = 10000
    seed: Optional[int] = None


def lbr_exploitability(
    game: Game,
    profile: StrategyProfile,
    config: Optional[LBRConfig] = None,
) -> float:
    """Approximate exploitability via Local Best Response sampling.

    For each sampled deal, plays out the game where:
    - The opponent follows the fixed profile
    - The BR player sees their private info and plays the locally optimal action

    This is an approximation — it computes a best response that treats each
    information set independently (ignoring that BR actions at one infoset
    affect reach probabilities at downstream infosets). The true exploitability
    is <= LBR exploitability, so LBR provides an upper bound.

    For exact results on small games, use exploitability() instead.
    """
    config = config or LBRConfig()
    rng = random.Random(config.seed)

    br0 = _lbr_value(game, profile, br_player=0, num_samples=config.num_samples, rng=rng)
    br1 = _lbr_value(game, profile, br_player=1, num_samples=config.num_samples, rng=rng)
    return 0.5 * (br0 + br1)


def _lbr_value(
    game: Game,
    profile: StrategyProfile,
    br_player: int,
    num_samples: int,
    rng: random.Random,
) -> float:
    """Estimate BR value for one player via sampling."""
    total_value = 0.0

    for _ in range(num_samples):
        total_value += _lbr_sample(game, profile, br_player, game.initial_state(), rng)

    return total_value / num_samples


def _lbr_sample(
    game: Game,
    profile: StrategyProfile,
    br_player: int,
    state,
    rng: random.Random,
) -> float:
    """Sample one trajectory and return BR player's utility."""
    if game.is_terminal(state):
        return game.terminal_utility(state, br_player)

    if game.current_player(state) == -1:
        outcomes = game.chance_outcomes(state)
        actions, probs = zip(*outcomes)
        r = rng.random()
        cumulative = 0.0
        chosen = actions[0]
        for a, p in outcomes:
            cumulative += p
            if r < cumulative:
                chosen = a
                break
        return _lbr_sample(game, profile, br_player, game.next_state(state, chosen), rng)

    player = game.current_player(state)
    actions = game.legal_actions(state)

    if player == br_player:
        # BR player: try all actions and pick the best (locally optimal)
        best_value = float('-inf')
        for a in actions:
            v = _lbr_sample(game, profile, br_player, game.next_state(state, a), rng)
            if v > best_value:
                best_value = v
        return best_value
    else:
        # Opponent: follow the profile
        key = game.infoset_key(state, player)
        action_probs = profile.get(key, {})
        probs = [action_probs.get(a, 1.0 / len(actions)) for a in actions]
        r = rng.random()
        cumulative = 0.0
        chosen = actions[0]
        for a, p in zip(actions, probs):
            cumulative += p
            if r < cumulative:
                chosen = a
                break
        return _lbr_sample(game, profile, br_player, game.next_state(state, chosen), rng)


def per_infoset_exploitability(
    game: Game,
    profile: StrategyProfile,
    br_player: int,
) -> Dict[str, float]:
    """Compute exploitability contribution per information set.

    Returns a dict mapping infoset keys to their contribution to the
    total best-response value. Useful for identifying which infosets
    have the weakest play.
    """
    state_reach: Dict = {}
    infoset_states: Dict[str, List] = {}
    infoset_actions: Dict[str, List] = {}

    def collect(state, reach_opp: float) -> None:
        state_reach[state] = reach_opp
        if game.is_terminal(state):
            return
        if game.current_player(state) == -1:
            for outcome, prob in game.chance_outcomes(state):
                collect(game.next_state(state, outcome), reach_opp * prob)
            return
        player = game.current_player(state)
        actions = game.legal_actions(state)
        if player == br_player:
            key = game.infoset_key(state, player)
            infoset_states.setdefault(key, []).append(state)
            infoset_actions.setdefault(key, actions)
            for a in actions:
                collect(game.next_state(state, a), reach_opp)
        else:
            key = game.infoset_key(state, player)
            action_probs = profile.get(key, {})
            for a in actions:
                p = action_probs.get(a, 1.0 / len(actions))
                collect(game.next_state(state, a), reach_opp * p)

    collect(game.initial_state(), 1.0)

    value_cache: Dict = {}

    def state_value(state) -> float:
        if state in value_cache:
            return value_cache[state]
        if game.is_terminal(state):
            v = game.terminal_utility(state, br_player)
        elif game.current_player(state) == -1:
            v = 0.0
            for outcome, prob in game.chance_outcomes(state):
                v += prob * state_value(game.next_state(state, outcome))
        else:
            player = game.current_player(state)
            actions = game.legal_actions(state)
            if player == br_player:
                key = game.infoset_key(state, player)
                totals = [0.0] * len(actions)
                for s in infoset_states[key]:
                    for idx, a in enumerate(actions):
                        totals[idx] += state_reach[s] * state_value(game.next_state(s, a))
                best_idx = max(range(len(actions)), key=lambda i: totals[i])
                v = state_value(game.next_state(state, actions[best_idx]))
            else:
                key = game.infoset_key(state, player)
                action_probs = profile.get(key, {})
                v = 0.0
                for a in actions:
                    p = action_probs.get(a, 1.0 / len(actions))
                    v += p * state_value(game.next_state(state, a))
        value_cache[state] = v
        return v

    # Pre-compute all values
    state_value(game.initial_state())

    # Compute per-infoset contribution
    contributions: Dict[str, float] = {}
    for key, actions in infoset_actions.items():
        totals_br = [0.0] * len(actions)
        totals_profile = [0.0] * len(actions)
        for state in infoset_states[key]:
            for idx, a in enumerate(actions):
                child_val = state_value(game.next_state(state, a))
                totals_br[idx] += state_reach[state] * child_val
                action_probs = profile.get(key, {})
                p = action_probs.get(a, 1.0 / len(actions))
                totals_profile[idx] += state_reach[state] * child_val * p

        br_val = max(totals_br)
        profile_val = sum(totals_profile)
        contributions[key] = br_val - profile_val

    return contributions
